<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>CuSha</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://farkhor.github.io/CuSha/"><img src="images/CuSha-logo.png"></img></a></h1>
        <p>CuSha is a CUDA-based vertex-centric graph processing framework that uses G-Shards and CW representations.</p>

        <p class="view">View the Project on GitHub: <a href="https://github.com/farkhor/CuSha">farkhor/CuSha</a></p>


        <ul>
          <li><a href="https://github.com/farkhor/CuSha/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/farkhor/CuSha/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/farkhor/CuSha">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        
    
    <h3><a name="about-cusha" class="anchor" href="#about-cusha"><span class="octicon octicon-link"></span></a>About CuSha</h3>    
      <p>
        CuSha is a CUDA-based vertex-centric graph processing framework that uses G-Shards and Concatenated Windows (CW) representations to store graphs inside the GPU global memory. 
        G-Shards and CW consume more space compared to Compressed Sparse Row (CSR) format but on the other hand provide better performance due to GPU-friednly representations. 
        For completeness, provided package also includes Virtual Warp-Centric (VWC) processing method for GPU and a multi-threaded CPU implementation, both using CSR representation. CPU implementation utilizes Pthreads.
      </p>
      <p>
      	You can download the paper about CuSha from <a href="http://www.cs.ucr.edu/~fkhor001/projects.html#cusha">here</a>. You can also download the slides I prepared for this paper from <a href="http://www.cs.ucr.edu/~fkhor001/CuSha/CuSha_Slides.pptx">here</a>. You'll need MS PowerPoint '07 or higher for a proper presentation.
      </p>
        
    <h3><a name="requirements" class="anchor" href="#requirements"><span class="octicon octicon-link"></span></a>Requirements</h3>    
      <p><ul>
        <li>Provided package, as it is, contains multiple source files. 
        As a result, you will need CUDA 5.0 or higher to compile them separately and link, alongside a CUDA-enabled device with Compute Capability (CC) 2.0 or higher to run. 
        For more info, please have a look at <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/gpu-object-linking.pdf">these slides</a>.    
        </li><li>For processing with both G-Shards and CW representations, I have used  <em>__syncthreads _or(int)</em> function, which is not supported in architectures before Fermi. 
        Furthermore, atomic operations require minimum CCs. 
        Please refer to <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/#atomic-functions">CUDA programming guide</a> for more info. 
        </li><li>CuSha is forward-compatible with devices having a physical warp size rather than 32.   
        </li><li>Provided VWC on GPU requires physical warp size to be 32.   
        </li><li>Multi-threaded CPU program requires <em>NVCC</em> to be compiled although with small modifications it can be a pure C/C++ piece of code.  
        </li><li>This package is tested under <b>Ubuntu 12.04</b>. It wouldn't be hard to port it to other Operating Systems.
     </li>
     	
     </ul></p>    
        
    <h3><a name="usage" class="anchor" href="#usage"><span class="octicon octicon-link"></span></a>Usage</h3>    
     <p>
       Now I assume your CUDA-enabled device meets the requirements and you want to use CuSha. 
       This section tells you how to define your algorithm in CuSha, compile them, and run them. 
       </p>
     <h4>Algorithm Definition</h4>
     <p>
       First of all, <b>Please note algorithm definition steps for CSR based graph processing (VWC and Multi-threaded CPU) are a bit different but completely independant from CuSha representations.</b> 
       It means if you are defining your algorithm for CSR-based processing, processing procedure with CuSha representations stay unchanged (and also vice versa). 
       </p>
       <p>
       To use CuSha, you need to first specify vertex content and edge content for the graph. 
       For CSR based processing methods, fill up <em>csr-UserStructures.cuh</em>, 
       and for G-Shards and CW fill up <em>cusha-UserStructures.cuh</em> For example, for SSSP algorithm, <em>Vertex</em> and <em>Edge</em> structures are defined like below:
<pre><code>typedef struct Vertex{ unsigned int distance; }Vertex;
typedef struct Edge{ unsigned int weight; }Edge;    
</code></pre>
    </p>
    <p>You can also fill up <em>Vertex_static</em> structure when there are vertex properties remaining intact during graph processing.</p>

    <p>The next step is to define required functions for CSR-based processing and CuSha. First I explain about CuSha functions, and then about CSR ones. 
    Functions you need to modify to process your graph with CuSha are all inside <em>cusha-UserFunctions.cuh</em>. Processing with CuSha happens in 4 step from which 3 are explicitly and directly determined by the user. 
    The first fucntion controllong the first step is <b>init_compute_CuSha</b> that initializes vertex array in shared memory. This shared memory vertex array is used as an scratchpad (just like what they say about GPU shared memory) to perform computation on vertices. 
    The role of <b>init_compute_CuSha</b> function is to properly make shared memory ready for the current iteration. As an example, for our SSSP, the function is:
<pre><code>inline __device__ void init_compute_CuSha(	Vertex* local_V,	// Address of the corresponding vertex in shared memory.
	Vertex* V	) {	// Address of the vertex in global memory
		local_V->distance = V->distance;
}
</code></pre>
</P>
<P>
    In the next step, which happens in the function <b>compute_CuSha</b> every thread is assigned to process one edge. The operation done by the thread atomically updates the Vertex array in shared memory. 
    Below is <b>compute_CuSha</b> function for our SSSP. 
<pre><code>inline __device__ void compute_CuSha(	Vertex* SrcV,	// Source vertex in global memory.
	Vertex_static* SrcV_static,	// Source Vertex_static in global memory. Dereferencing this pointer if it's not defined causes error.
	Edge* E,	// Thread's specific shared memory.
	Vertex* local_V	) {	// Current value of the corresponding destination vertex.
		if (SrcV->distance != CUSHA_SSSP_INF)	// Just to prevent possible unpredicted overflows.
			atomicMin ( &(local_V->distance), SrcV->distance + E->weight );
}
</code></pre>
    One thing to note is that passed addresses must not be dereferenced if they are not already defined in structures.
    </p>
    <p>
    The third step is to signal the caller function if the newly calculated vertex value is eligible to update the previosu value.
    <b>update_condition_CuSha</b> function does it by returning a boolean. This function for SSSP can be defined like below:
<pre><code>inline __device__ bool update_condition_CuSha(	Vertex* local_V,	// newly calculated vertex content.
	Vertex* V	) {	// Vertex content at the end of previous iteration.
		return ( local_V->distance < V->distance );
}
</code></pre>
Please note returned value will consequently signal the host to launch new GPU kernels, so make sure your algorithm converges by <b>update_condition_CuSha</b> definition.
    </p>
    <p>
    Beside above three functions there are two other user-specified functions in the same file that control initialization of the graph and how outputs should be.
    <b>completeEntryCuSha</b> function is called during pre-processing for every edge and initalizes the edge content or source/destination vertex content appropriately. Examples can be found in templates.
    <b>printVertexOutputCuSha</b> function is called after the processing has finished for every vertex to write the vertex content to the output file.
    </p>
    <p> For CSR-based graph processing (using VWC or MTCPU methods), you need to modify the content of the file <em>csr-UserFunctions.cuh</em>. 
    Most of the functions are similar to what I already explained except that computation function in the second step is now replaced with two other functions. 
    One is <b>compute_local</b> in which a thread calculates and nominates a new value for the vertex. Then in <b>compute_reduce</b> function, contents are compared two-by-two. 
    This reduction happens for all the neighbors of a vertex, and the last selected nominee will be sent for update verification in <b>update_condition</b> function.
    </p>
    <h4>Compile and Link</h4>
    <p>
      I have provided a very straight forward build bash script named <em>sfBuild.sh</em> that compiles files with required flags and links them and outputs the executable.
      Specified architecture (compute capability) in <em>sfBuild.sh</em> is 3.5. You will have to modify it according to your target device.
    </p>
    <p>
    Provided build file does compilation for all the files every time you run it. 
    If you are interested to play with the code, it is definitely better to use <b>Nsight Eclipse Edition</b> provided in Nvidia toolkit for non-windows operating systems. 
    In order to do so, create an empty CUDA C/C++ project (with specified sm architecture and separate compilation) in Nsight. 
    Then right click on the project on <em>Project Explorer</em>, select <em>import</em>-><em>General</em>-><em>File System</em>. 
    Then you have to specify the directory you have downloaded from this website and check <em>entryPoint.cu</em> plus all other 3 folders. 
    After you click on finish, you are ready to build the project for Debug and Release and tune project's settings easier. 
    </p>
    
    <h4>Run</h4>
    <p>If you run the output executable from the last step without any command-line input (or without required parameters), it prints out the usage to the system standard output. 
    That's pretty much what I'm about to write here, except I don't bring it here anymore because you already know how to get what I was going to say ;) . 
    One thing I have to explain a bit about is the input file to the executable. An input graph for CuSha (in form of a plain text file) contains edges.
    Each line in the input file corresponds to an edge in the graph and has at least two vertex indices separated by space or tab. 
    The first number specifies the vertex index the tail of the edge corresponds to and the second one shows the vertex index for the head. 
    This convention is found in many publicly available real-world graphs, such as <a href="http://snap.stanford.edu/data/">SNAP</a>'s. 
    Additinal inputs for edges like edge weight or initial vertex content can be supplied with more numbers in each row. 
    Examples can be found in provided templates.
    </p>
    <p>
    	One last note about input files. Although vertex indices are inherently abstract concepts, they're important in the input files. Be careful that if a graph only has one edge connecting the vertex with index 27827 to the vertex with index 26742, for example, in the eye of the provided package, it has up to 27827 vertices. Plus since I used unsigned integer type to hold vertex indices, program's behavior when faced with indices greater than the amount an unsigned int can hold is undefined. 
    </p>
    <p>
    	If a graph has large vertex indices (for example <a href="https://snap.stanford.edu/data/cit-HepPh.html">this one</a>) and CuSha fails to come up with a proper block size on its own, you can use <a href="https://github.com/farkhor/Graph-Edge-List-Vertex-Index-Minimizer">this program</a> to minimize the indices, without changing the graph structure.
    </p>
    
    <h3>Things to note</h3>
    <p><ul>
    	<li>
    	CuSha heavily relies on atomic operations on shared memory. As explained by <em>Mark Harris</em> in <a href="https://devblogs.nvidia.com/parallelforall/5-things-you-should-know-about-new-maxwell-gpu-architecture/">this post</a>, Maxwell architecture provides native shared memory atomics which will defintiely make CuSha much faster. Maxwell also has bigger shared memory capacity per SM that will make it possible to have bigger shards and windows in CuSha, hence accelerating the whole procedure.
    	</li>
    	<li>
    	We have not taken into account memory allocations in time measurements for CPU and GPU copying/processing methods.
    	</li>
    </ul></p>
    
    <h3>Acknowledgements</h3>
    <p>
    	This work is supported by National Science Foundation grants CCF-1157377 and CCF-0905509 to UC Riverside.
    </p>
    
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/farkhor">farkhor</a>.</p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
